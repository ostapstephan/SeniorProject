\documentclass[12pt,conference,onecolumn]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage[normalem]{ulem}

%\usepackage[citestyle=ieee,backend=biber]{biblatex}
%\addbibresource{bibliography.bib}

% \usepackage[ruled]{algorithm2e}
% \SetKwProg{Fn}{Function}{:}{}
% \SetAlgoFuncName{Ftn.}{Function}
% \SetAlgorithmName{Alg.}{Algorithm}{List of Algorithms}

\begin{document}

\title{$Eye_tRack$: Project Proposal for an Eye-Tracking System}
\author{
	\IEEEauthorblockN{Stephen Brett}
	\IEEEauthorblockA{The Cooper Union\\New York, NY, USA\\Email: brett@cooper.edu}
	\and
	\IEEEauthorblockN{Daniel Nakhimovich}
	\IEEEauthorblockA{The Cooper Union\\New York, NY, USA\\Email: nakhimov@cooper.edu}
	\and
	\IEEEauthorblockN{Ostap Voynarovskiy}
	\IEEEauthorblockA{The Cooper Union\\New York, NY, USA\\Email: voynarov@cooper.edu}
}

\maketitle
\renewcommand{\baselinestretch}{1.5} 
\begin{abstract}
\textbf{\\Project dates: September 2018 -- December 2018\\}
\textnormal{
	Abstract
}
\end{abstract}
%\onecolumn \maketitle \normalsize \vfill
\IEEEpeerreviewmaketitle

\section{Project Description} \label{sec:proj_desc}
The project that we propose is a desktop eye tracking system. The functional goal of the system is to determine the direction in which a person is looking while sitting at a desk. The purpose of this device \sout{will be to pass butter; oh my god} will be to help with productivity. The simplest application of the device would be to simply keep a record of where ones eyes are looking so that one could go back and analyze how much time was spent on various tasks or to provide an estimate of focus. A more interesting application would be to actively control other devices on your desk, namely -- but not limited to -- the computer. For example, by tracking where a user is looking at the computer screen, you can change the focus on different application windows or select different fields on an online form. With high enough accuracy one could potentially control a cursor this way as well. In addition to computer control, if you have other desktop accessories that are either connected to the computer or through a network then you could activate certian functions on these devices as well. For example, one could turn on a usb powered desk lamp, check notifications on their phone by glancing at it, or activate their IoT teapot and give a whole new meaning to the phrase, "watching water boil". Also, although we are focusing on a desktop mounted system, the technology behind the eye tracker would be easily adaptable to other environments and thus useful for a host of applications such as human intent aware robots, driver focus in a car, anti-staring neclaces that automatically say "My eyes are up here!". The applications are really only limited by the imagination and the relative speed/distance of the subject being observed to the eye tracking system.

\section{Background} \label{sec:background}
\subsection{Eye Tracking}
Eye gaze tracking (EGT) systems are used to estimate the location of the point at which a person is looking. There are both intrusive and non-intrusive methods of taking the necessary measurements. Intrusive methods involve physical contact of the tracking system with the user. One such method uses sceleral search coils, which are contact lenses that have been modified to have a coil of wire embedded in them. In a magnetic field, the position of the coil, and thus the direction in which the eye is looking, can easily and quickly be determined. This method is generally considered to be the most accurate, and is often used in medical research \cite{chennamma}. Yet since this method is invasive, requiring the insertion of a device into the body, it is not practical for commercial use. An example of intrusive tracking that is not invasive is electro-oculography, in which sensors, are adhered to the skin in the area around the eyes, and detect differences in electric potential caused by eye rotation. This is much less expensive, but is also less accurate, only to $2^{\circ}$ \cite{morimoto}.

A method of eye tracking that is conducive to everyday use involves video processing of the user's face and eyes. This can be either intrusive or non-intrusive, or even both, depending on where the cameras are mounted. For an intrusive approach, the user would wear a headset with one or more cameras attached, possibly along with light sources. With a non-intrusive method, the camera would be mounted remote from the user, giving them more freedom of motion. For this project, we are going to focus on video-based eye gaze tracking.

\subsection{Video-Based Eye Tracking}
The feature that is most commonly tracked by video-based systems is the pupil of the user's eye (assuming that both eyes point in the same direction). In order to increase the contrast between the iris and pupil, and thus more easily perform edge detection to isolate the pupil and determine its shape, additional light sources are used . These can produce IR, near-IR, or visible light. IR is much more commonly used, since light at those wavelengths is invisible to the user, and will not be distracting. Using near-IR light allows the light reflected from the eye to be detected by most commercial video cameras \cite{morimoto}. With some additional modifications to the cameras, they can be made to be more sensitive to those wavelengths of light. 

Depending of the placement of the light source with respect to the camera, different kinds of reflected light may be captured. If the IR light source is placed near the camera, the path from the source to the eye, the optical axis of the light source, will be similar to the optical axis of the camera. This has the effect of capturing the IR light that is reflected off the retina at the back of the eye, which then shines through the pupil, making the pupil appear bright. This is known as the bright pupil effect. This technique greatly increases the contrast between the iris and the pupil, making the edge detection faster and more accurate. This also allows the system to work across a range of ambient light intensities. However, the intensity of the bright pupil effect varies quite a lot between test subjects \cite{morimoto}. Thus, this method on its own is not very reliable.

If the IR light source is placed further away from the camera, the reflected light will appear as a glint in the eye, a phenomenon known as the dark pupil effect. This glint is also known as a corneal reflection (CR). The CR is then used as a reference point for calculating the gaze direction. The center of the pupil is found, and the vector between the center and the CR is found \cite{morimoto}. Some methods make use of both the bright and dark pupil effects, combining the better edge detection of the bright pupil method with the subject-independence of the dark pupil method to achieve greater accuracy. However, this requires multiple cameras and a more complex algorithm, which increases the latency of the system \cite{chennamma}. 

The determination of the gaze direction can be accomplished using either feature-based or appearance-based methods. Feature-based methods take into account the measurements of the features of the eye discussed above. The mappings of the features to the gaze direction can be based on a three-dimensional geometric model of the eye, from which the optical and visual axes of the eye are constructed, and the intersection of those vectors with the surroundings is found. These models are less sensitive to variations in lighting and viewpoint \cite{chennamma}. However, calibration is typically required to determine mapping between the measurements taken and the actual orientation of the eye. Else, there will be an approximately fixed error that will vary from user to user \cite{morimoto}. Also included in the category of feature-based methods are those that assume that assume a relationship between the features and the gaze coordinates is of a certain parametric or non-parametric form, and then use techniques such as polynomial regression or neural networks, respectively, to determine the gaze direction \cite{chennamma}.

Another class of techniques, known as appearance-based methods, do not even make use of those geometric features discussed. Instead, the system is trained with a large amount of data, in each entry of which the appearance of the user's eyes is associated with a set of gaze coordinates. The model derived from this learning is generalized to all users \cite{morimoto}.
\subsection{Future Research}

As further research is performed, it will be necessary to learn more about specific methods for detecting and tracking the head and eyes, in order to know where to look to gather the necessary data, so that computation time is not wasted on excess features. The noise, both from the user's surroundings (i.e. monitor, room lighting) and from IR light sources, must be removed from the eye image, in order to perform accurate feature detection. Algorithms for feature detection, as well as those for mapping those features to gaze coordinates, will need to be both efficient and accurate. There is a variety of system configurations, of cameras and light sources. It will be necessary to find and test multiple setups. 
\section{Proposition} \label{sec:proposition}
There are quite a few different types of approaches we can take to implement an eye tracking system, but we believe that trying to achieve good performance will require significant experimentation. The approaches we have seen have varied significantly.  We first explore the options then present a direction for our research.

	While taking advantage of the bright pupil/red eye effect is quite effective for determining whether a user at an arbitrary distance is looking at any precise location, it will not be able to present any useful information  about the user’s gaze outside of that direct position.  Thus, we are going to approach the problem of eye tracking by using the dark pupil approach.
	
	The dark pupil approach relies most heavily on having a high resolution image of the pupil. By finding the center of the pupil, and comparing that to points of reflected light on the eyeball, we can back calculate the user’s gaze. To avoid having the user be distracted by bright lights, we will be using an array of infrared LEDs. IR light is not only invisible to humans, but also capable of passing through visible light filters which can aid with noise reduction. 
	
	The problem of tracking gaze can be broken down into: figuring out the location of the head, capturing high resolution footage of the pupil, locating the reflection of the IR light on the pupils, then stitching all this information together in a productive way to achieve good precision and accuracy of the user’s gaze. To locate the specific position of the user’s head, we can make the user wear something of a known dimension allowing us to use only one camera mounted on the monitor for locating the head relative to the camera. Alternatively, we can have two cameras mounted at a known distance apart  from one another, and create a depth map from that, then use that to locate the user’s head. This will be accomplished by locating similar edges in the simultaneous frames, then using geometry to back calculate the location of the user’s head. 
	
	Once we have head position, we now need to capture high resolution video of the pupil. Ideally, the web cameras that we will be using should be able to provide a clear image of the eyeball and the reflected IR light. If this is the case then we can move on to building an array of IR LEDs. Although, if we are unable to achieve high enough resolution from just the mounted cameras, we may need to either build a headset that can view the eyeball more accurately, or find a way to optically zoom in on the eyeball. This becomes a complexity vs ergonomics trade off. A web cam with an optical zoom will require a way to keep it pointed at the eyes which will increase complexity, while a head mounted solution will have to be ergonomic for many people to use and rely much more heavily on the screen mounted camera to calculate the exact position of the headpiece. 
	
	Once we have the ability to collect good data about where the user’s eyes and head are we can attempt to make sense of the data. If the position data that we collect is sufficiently good, then it would be reasonable to assume that using geometry should be able to get us decent results. However if this is not the case, we could attempt to use regression or computational graphs to gain higher accuracy. 
	
	To evaluate the model, we can have a user look at a point and then compare the known location to the location that our system will output and calculate error based off that. Additionally, we could test the accuracy of the head locating system by using a lidar module and comparing its output to the output of our system.

\bibliographystyle{IEEEtran}
\bibliography{bibliography}
%\printbibliography

\end{document}
