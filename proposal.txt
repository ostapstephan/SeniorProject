	There are quite a few different types of approaches we can take to implement an eye tracking system, but we believe that trying to achieve good performance will require significant experimentation. The approaches we have seen have varied significantly.  We first explore the options then present a direction for our research.
	While taking advantage of the bright pupil/red eye effect is quite effective for determining whether a user at an arbitrary distance is looking at any precise location, it will not be able to present any useful information  about the user’s gaze outside of that direct position.  Thus, we are going to approach the problem of eye tracking by using the dark pupil approach.
	The dark pupil approach relies most heavily on having a high resolution image of the pupil. By finding the center of the pupil, and comparing that to points of reflected light on the eyeball, we can back calculate the user’s gaze. To avoid having the user be distracted by bright lights, we will be using an array of infrared LEDs. IR light is not only invisible to humans, but also capable of passing through visible light filters which can aid with noise reduction. 
	The problem of tracking gaze can be broken down into: figuring out the location of the head, capturing high resolution footage of the pupil, locating the reflection of the IR light on the pupils, then stitching all this information together in a productive way to achieve good precision and accuracy of the user’s gaze. To locate the specific position of the user’s head, we can make the user wear something of a known dimension allowing us to use only one camera mounted on the monitor for locating the head relative to the camera. Alternatively, we can have two cameras mounted at a known distance apart  from one another, and create a depth map from that, then use that to locate the user’s head. This will be accomplished by locating similar edges in the simultaneous frames, then using geometry to back calculate the location of the user’s head. 
	Once we have head position, we now need to capture high resolution video of the pupil. Ideally, the web cameras that we will be using should be able to provide a clear image of the eyeball and the reflected IR light. If this is the case then we can move on to building an array of IR LEDs. Although, if we are unable to achieve high enough resolution from just the mounted cameras, we may need to either build a headset that can view the eyeball more accurately, or find a way to optically zoom in on the eyeball. This becomes a complexity vs ergonomics trade off. A web cam with an optical zoom will require a way to keep it pointed at the eyes which will increase complexity, while a head mounted solution will have to be ergonomic for many people to use and rely much more heavily on the screen mounted camera to calculate the exact position of the headpiece. 
	Once we have the ability to collect good data about where the user’s eyes and head are we can attempt to make sense of the data. If the position data that we collect is sufficiently good, then it would be reasonable to assume that using geometry should be able to get us decent results. However if this is not the case, we could attempt to use regression or computational graphs to gain higher accuracy. 
	To evaluate the model, we can have a user look at a point and then compare the known location to the location that our system will output and calculate error based off that. Additionally, we could test the accuracy of the head locating system by using a lidar module and comparing its output to the output of our system.

	


https://www.intersil.com/content/dam/Intersil/documents/an17/an1737.pdf
